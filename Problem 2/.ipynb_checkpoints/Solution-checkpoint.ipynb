{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "\n",
    "\n",
    "NodeList ={}\n",
    "class Node:\n",
    "    def __init__(self,id):  \n",
    "        self.id = id\n",
    "        self.Children = []\n",
    "        self.visited = False\n",
    "\n",
    "        \n",
    "    def AddChild(self,cid):\n",
    "        self.Children.append(cid)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_node(id):\n",
    "        global NodeList\n",
    "        if id not in NodeList:\n",
    "            NodeList[id] = Node(id)\n",
    "        return NodeList[id]\n",
    "    \n",
    "class BFS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self,edges):\n",
    "        global NodeList\n",
    "        for e in edges:\n",
    "            node1 = Node.get_node(e[0])\n",
    "            node2 = Node.get_node(e[1])\n",
    "            node1.AddChild(e[1])\n",
    "            node2.AddChild(e[0])\n",
    "        \n",
    "        \n",
    "        self.idlist = {}\n",
    "        for nid in NodeList:\n",
    "            self.idlist[nid] = 1\n",
    "\n",
    "        count = 0\n",
    "        while len(self.idlist) > 0:\n",
    "            print(count)\n",
    "            start = self.idlist[list(self.idlist.keys())[0]]\n",
    "            que = [start]\n",
    "            count += 1\n",
    "            while len(que) > 0:\n",
    "                nque = []\n",
    "                for nid in que:\n",
    "                    if nid in self.idlist:\n",
    "                        del self.idlist[nid]\n",
    "                        node = Node.get_node(nid)\n",
    "                        nque += node.Children\n",
    "                que = nque[:]\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nodes = {}\n",
    "class Processor:\n",
    "    def __init__(self,fname):\n",
    "        self.Nodes = {}\n",
    "        self.edges = []\n",
    "        self.prev_lowest_vertices = []\n",
    "        ifile = open(fname)\n",
    "        c = 0\n",
    "        for l in ifile:\n",
    "            c += 1\n",
    "            if c > 100:\n",
    "                break\n",
    "            l = l.strip()\n",
    "            if len(l) < 1:\n",
    "                continue\n",
    "            l = l.split(' ')\n",
    "            fnode,snode = int(l[0]),int(l[1])\n",
    "            if fnode < snode:\n",
    "                fnode,snode = snode,fnode\n",
    "            self.edges.append((fnode,snode))\n",
    "        self.prev_lowest_vertices = list(set([e[1] for e in self.edges]))\n",
    "        self.prev_lowest_vertices.sort()\n",
    "            \n",
    "            \n",
    "    def CreateNodes(self,edges):\n",
    "        for e in edges:\n",
    "            self.Nodes[e[0]] = e[1]\n",
    "            \n",
    "    def is_reduced(self,edges):\n",
    "        lowest_vertices = list(set([e[1] for e in edges]))\n",
    "        lowest_vertices.sort()\n",
    "\n",
    "        if lowest_vertices == self.prev_lowest_vertices:\n",
    "            return False\n",
    "        print(len(self.prev_lowest_vertices),len(self.prev_lowest_vertices) -len(lowest_vertices) )\n",
    "        self.prev_lowest_vertices = lowest_vertices\n",
    "        return True\n",
    "    \n",
    "    def start(self):\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        edges = sc.parallelize(self.edges)\n",
    "        reducer = Reducer()\n",
    "        edges = reducer.reduce(edges)   \n",
    "        self.CreateNodes(edges)\n",
    "        while True:\n",
    "            if not self.is_reduced(edges):\n",
    "                print(len(self.prev_lowest_vertices))\n",
    "                return\n",
    "            mapper = Mapper()\n",
    "            edges = mapper.map(edges,self.Nodes)\n",
    "            edges = reducer.reduce(edges)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "class Mapper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def map(self,edges,Nodes):\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        edges = sc.parallelize(edges)\n",
    "        return edges.map(lambda x:(x[1] if x[1] in Nodes else x[0],Nodes[x[1]] if x[1] in Nodes else x[1]))\n",
    "    \n",
    "\n",
    "\n",
    "class Reducer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reduce(self,m):\n",
    "        m = m.groupByKey().map(lambda x:(x[0],min(x[1])))\n",
    "        return m.collect()\n",
    "    \n",
    "    \n",
    "p = Processor('test.txt')\n",
    "bfs = BFS()\n",
    "bfs.run(p.edges)\n",
    "#p.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
